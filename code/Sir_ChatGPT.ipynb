{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr5l1yGMd1mrOuucAltEzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Sir-ChatGPT/blob/main/code/Sir_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q4jqlBf3HRnQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "import io\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotinas utilitárias"
      ],
      "metadata": {
        "id": "2HsZlZ7YlPi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mostra_dict(dicionario: dict):\n",
        "    \"\"\"\n",
        "    Imprime informações sobre o dicionário recebido como parâmetro.\n",
        "\n",
        "    Argumentos:\n",
        "    - dicionario: um dicionário a ser impresso\n",
        "\n",
        "    Retorna:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # obtém a primeira e última chave do dicionário\n",
        "    primeiro_elemento = list(dicionario.keys())[0]\n",
        "    ultimo_elemento = list(dicionario.keys())[-1]\n",
        "\n",
        "    # imprime o tamanho do dicionário e as informações sobre seus limites\n",
        "    print(f\"O dicionário tem tamanho: {len(dicionario)}\")\n",
        "    print(f\"Seus limites:\\n {primeiro_elemento}:\\n {dicionario[primeiro_elemento]},\\n {ultimo_elemento}:\\n {dicionario[ultimo_elemento]}\")\n"
      ],
      "metadata": {
        "id": "ctvgk_oNlUPj"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 1: Coleta dos dados"
      ],
      "metadata": {
        "id": "2L6Y2H8MH14s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baixar coleção CISI.ALL"
      ],
      "metadata": {
        "id": "sZ3pW9-8L6ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "6NSNoe3EI-gL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a URL da coleção CISI\n",
        "url = \"http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\"\n",
        "\n",
        "# Define o caminho onde o arquivo será salvo\n",
        "file_path = \"cisi.tar\"\n",
        "\n",
        "# Faz o download do arquivo\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Extrai os arquivos da coleção CISI\n",
        "with tarfile.open(file_path, \"r\") as tar:\n",
        "    tar.extractall()\n"
      ],
      "metadata": {
        "id": "-QFPkX52Jzts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove('cisi.tar.gz')\n",
        "os.remove('cisi.tar')"
      ],
      "metadata": {
        "id": "5wYycqvbMTGH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o CISI.ALL\n",
        "\n",
        "Este código carrega o arquivo CISI.ALL em um dicionário chamado documents, onde as chaves são os IDs dos documentos e os valores são dicionários com os campos \"title\", \"author\" e \"text\". Por exemplo, para acessar o título do documento de ID 1, basta fazer documents[1][\"title\"]"
      ],
      "metadata": {
        "id": "2UwCT9VIN2XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GfrdtmwQSHO",
        "outputId": "920fda36-8caf-4d87-8fe3-f24887dabb70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1460"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_cisi_arquivo(nome_arquivo:str, se_debug:bool=False):\n",
        "\n",
        "  # abre o arquivo CISI.ALL em modo de leitura\n",
        "  with open(nome_arquivo, \"r\") as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "  # inicializa o dicionário\n",
        "  documents = {}\n",
        "\n",
        "  # inicializa as variáveis de controle\n",
        "  doc_id = None\n",
        "  field = None\n",
        "  text = \"\"\n",
        "\n",
        "  # percorre as linhas do arquivo\n",
        "  for line in lines:\n",
        "    # se a linha começa com \".I\", então é um novo documento\n",
        "    if line.startswith((\".I\", \".T\", \".A\", \".W\", \".X\", \".B\")):\n",
        "      # faça algo se a linha começar com um desses valores\n",
        "        if doc_id is not None and field != \"id\":\n",
        "            documents[doc_id][field] = text.strip()\n",
        "            if se_debug:\n",
        "              print(f\"Atribuido a documents[{doc_id}][{field}] = {text.strip()}\")\n",
        "            text = \"\"\n",
        "    # se a linha começa com \".I\", então é um novo documento\n",
        "    if line.startswith(\".I\"):\n",
        "        # extrai o ID do documento da linha\n",
        "        doc_id = int(line.split()[1])\n",
        "        field = \"id\"\n",
        "        documents[doc_id] = {}\n",
        "        if se_debug:\n",
        "          print(f\"novo doc_id {doc_id}\")\n",
        "    # se a linha começa com \".T\", \".A\" ou \".W\", então é um novo campo\n",
        "    elif line.startswith(\".T\"):\n",
        "        field = \"title\"\n",
        "    elif line.startswith(\".A\"):\n",
        "        field = \"author\"\n",
        "    elif line.startswith(\".X\"):\n",
        "        field = \"reference\"\n",
        "    elif line.startswith(\".B\"):\n",
        "        field = \"bibliograph\"\n",
        "    elif line.startswith(\".W\"):\n",
        "        field = \"text\"\n",
        "    # caso contrário, é um texto que faz parte do campo atual\n",
        "    else:\n",
        "        text += line\n",
        "    if se_debug:\n",
        "      print(f'doc_id {doc_id} field {field}: {line}')\n",
        "      if doc_id == 4: break \n",
        "\n",
        "  # adiciona o último documento ao dicionário\n",
        "  if doc_id is not None:\n",
        "      documents[doc_id][field] = text.strip()\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "iv-OWG_3Le7o"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos = carregar_cisi_arquivo('CISI.ALL')"
      ],
      "metadata": {
        "id": "EgnfWs83SD8D"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(documentos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOsLbLd8XM-w",
        "outputId": "1730916a-2d8c-478c-ea4c-d30f0588f6d9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 1460\n",
            "Seus limites:\n",
            " 1:\n",
            " {'title': '18 Editions of the Dewey Decimal Classifications', 'author': 'Comaromi, J.P.', 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\", 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1'},\n",
            " 1460:\n",
            " {'title': 'Modern Integral Information Systems for Chemistry and Chemical Technology', 'author': 'Chernyi, A.I.', 'text': \"At the present time, about 15% of all the world publications of \\nscientific and technical literature relate to chemistry and chemical\\ntechnology.  Each year throughout the world more than 250,000\\ndocuments are published:  journal papers, specifications for authors'\\ncertificates and patents, scientific and technical reports, monographs,\\netc., and in the last twenty years the number of such documents has\\nincreased by an average of 9% a year.  In these scientific documents\\ninformation on 100-150 thousand new chemical compounds is published.\", 'reference': '347\\t1\\t1460\\n452\\t1\\t1460\\n1095\\t1\\t1460\\n1136\\t1\\t1460\\n1223\\t1\\t1460\\n1261\\t1\\t1460\\n1285\\t1\\t1460\\n1460\\t6\\t1460\\n1460\\t6\\t1460'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consultas = carregar_cisi_arquivo('CISI.QRY')"
      ],
      "metadata": {
        "id": "8sw1_cNLePHk"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(consultas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEWBFttvj1bF",
        "outputId": "762c2277-eaf7-480c-bc99-d88f601f3d82"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 112\n",
            "Seus limites:\n",
            " 1:\n",
            " {'text': 'What problems and concerns are there in making up descriptive titles?\\nWhat difficulties are involved in automatically retrieving articles from\\napproximate titles?\\nWhat is the usual relevance of the content of articles to their titles?'},\n",
            " 112:\n",
            " {'title': 'A Fast Procedure for the Calculation of Similarity Coefficients in\\nin Automatic Classification', 'author': 'Willett, P.', 'text': 'A fast algorithm is described for comparing the lists of terms representing\\ndocuments in automatic classification experiments.  The speed of the procedure\\narises from the fact that all of the non-zero-valued coefficicents for a given\\ndocument are identified together, using an inverted file to the terms in the\\ndocument collection.  The complexity and running time of the algorithm are\\ncompared with previously described procedures.', 'bibliograph': '(Information Processing & Management, Vol. 17, No. 2, 1981, pp. 53-60)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inicializa o dicionário de relevância por consulta\n",
        "relevancia_consulta = {}\n",
        "\n",
        "# abre o arquivo CISI.REL em modo de leitura\n",
        "with open('CISI.REL') as f:\n",
        "    # percorre as linhas do arquivo\n",
        "    for line in f.readlines():\n",
        "        # extrai o ID da consulta e do documento da linha\n",
        "        qry_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0])\n",
        "        doc_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
        "        \n",
        "        # adiciona o ID do documento na lista de relevância da consulta\n",
        "        if qry_id in relevancia_consulta:\n",
        "            relevancia_consulta[qry_id].append(doc_id)\n",
        "        else:\n",
        "            relevancia_consulta[qry_id] = []\n",
        "            relevancia_consulta[qry_id].append(doc_id)\n",
        "# Fonte de apoio: https://www.kaggle.com/code/razamh/some-changes-in-cisi-eda"
      ],
      "metadata": {
        "id": "MdN4v1nzg6AR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(relevancia_consulta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZMSV96MeO5R",
        "outputId": "c6ec6572-d2e7-44b0-e0e2-c3b2318310ef"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 76\n",
            "Seus limites:\n",
            " 1:\n",
            " [28, 35, 38, 42, 43, 52, 65, 76, 86, 150, 189, 192, 193, 195, 215, 269, 291, 320, 429, 465, 466, 482, 483, 510, 524, 541, 576, 582, 589, 603, 650, 680, 711, 722, 726, 783, 813, 820, 868, 869, 894, 1162, 1164, 1195, 1196, 1281],\n",
            " 111:\n",
            " [328, 422, 448, 485, 503, 509]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HUtvq4hfLEVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 2: Pré-processamento dos textos"
      ],
      "metadata": {
        "id": "e007dYSoJ-zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "#stemmer = SnowballStemmer('english')\n"
      ],
      "metadata": {
        "id": "KLONEn8nlxPF"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLgzi6qlt7UZ",
        "outputId": "70c13015-393c-43a6-caf5-09b35f6ce96f"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoFU0laIrmuY",
        "outputId": "f695b8a7-49cc-4bbf-babb-4dffbabb9831"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "def preprocessa_texto(texto,\n",
        "                      to_lower=True, \n",
        "                      remove_pontuacao=True,\n",
        "                      remove_stopwords=True,\n",
        "                      aplica_stemming=True,\n",
        "                      aplica_lematizacao=True):\n",
        "    \"\"\"\n",
        "    Função que realiza o pré-processamento de um texto.\n",
        "    \n",
        "    Parâmetros:\n",
        "    texto (str): Texto a ser pré-processado\n",
        "    to_lower (bool): Flag que indica se deve transformar o texto para lower case. Default: True\n",
        "    remove_pontuacao (bool): Flag que indica se deve remover a pontuação do texto. Default: True\n",
        "    remove_stopwords (bool): Flag que indica se deve remover as stop words do texto. Default: True\n",
        "    aplica_stemming (bool): Flag que indica se deve aplicar stemming no texto. Default: True\n",
        "    aplica_lematizacao (bool): Flag que indica se deve aplicar lematização no texto. Default: True\n",
        "    \n",
        "    Retorna:\n",
        "    str: Texto pré-processado\n",
        "    \"\"\"\n",
        "    # Transforma o texto em lower case\n",
        "    if to_lower:\n",
        "        texto = texto.lower()\n",
        "\n",
        "    # Remove pontuação\n",
        "    if remove_pontuacao:\n",
        "        texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    palavras = texto.split()\n",
        "\n",
        "    # Remove stop words\n",
        "    if remove_stopwords:\n",
        "        palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stop_words]\n",
        "        palavras = palavras_sem_stopwords\n",
        "\n",
        "    # Aplica stemming\n",
        "    if aplica_stemming:\n",
        "        palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras]\n",
        "        palavras = palavras_stemizadas\n",
        "\n",
        "    # Aplica lematização\n",
        "    if aplica_lematizacao:\n",
        "        palavras_lematizadas = [lemmatizer.lemmatize(palavra) for palavra in palavras]\n",
        "        palavras = palavras_lematizadas\n",
        "\n",
        "    return palavras\n"
      ],
      "metadata": {
        "id": "06S5t8XQpCoT"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessa_texto(\"This is an example of text.\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2HwAWsksS1W",
        "outputId": "090ef2a0-e322-40fc-bfaa-af61e2d1f117"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['exampl', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessa_texto(\"This is a simple text.\") == ['simpl', 'text']\n",
        "assert preprocessa_texto(\"Hello! My name is John. Nice to meet you!\") == ['hello', 'name', 'john', 'nice', 'meet']\n",
        "assert preprocessa_texto(\"We are learning about Natural Language Processing.\") == ['learn', 'natur', 'languag', 'process']\n",
        "assert preprocessa_texto(\"The quick brown fox jumps over the lazy dog.\") == ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "assert preprocessa_texto(\"To be, or not to be: that is the question.\") == [\"question\"]\n",
        "assert preprocessa_texto(\"I'm a developer at OpenAI. I love working with AI and NLP technologies!\") == ['im', 'develop', 'openai', 'love', 'work', 'ai', 'nlp', 'technolog']\n",
        "assert preprocessa_texto(\"The cat is on the mat.\") == ['cat', 'mat']\n",
        "assert preprocessa_texto(\"An investment in knowledge pays the best interest.\") == ['invest', 'knowledg', 'pay', 'best', 'interest']\n",
        "assert preprocessa_texto(\"The quick brown fox jumps over the lazy dog.\") == ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "assert preprocessa_texto(\"To be, or not to be: that is the question.\") == [\"question\"]\n",
        "assert preprocessa_texto(\"I'm a developer at OpenAI. I love working with AI and NLP technologies!\") == ['im', 'develop', 'openai', 'love', 'work', 'ai', 'nlp', 'technolog']\n",
        "assert preprocessa_texto(\"The cat is on the mat.\") == ['cat', 'mat']\n",
        "assert preprocessa_texto(\"An investment in knowledge pays the best interest.\") == ['invest', 'knowledg', 'pay', 'best', 'interest']\n"
      ],
      "metadata": {
        "id": "pv_l-rptsSzJ"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessa_texto(\"Hello World!\") == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello, World!!!\") == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello World\", remove_pontuacao=False) == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello World\", remove_stopwords=False) == ['hello', 'world']\n",
        "assert preprocessa_texto(\"I am running in the park\", aplica_stemming=False) == ['running', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", to_lower=False, remove_stopwords=False, aplica_stemming=False, aplica_lematizacao=False) == ['I', 'am', 'running', 'in', 'the', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", aplica_lematizacao=False) == ['run', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", to_lower=False) == ['i', 'run', 'park']\n"
      ],
      "metadata": {
        "id": "Ah6ynUi0qgpR"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WsE6S-GyBBf",
        "outputId": "fc040942-f57e-40a1-81b2-4fbf5802203a"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': '18 Editions of the Dewey Decimal Classifications',\n",
              " 'author': 'Comaromi, J.P.',\n",
              " 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\",\n",
              " 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1'}"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessa_texto_em_dict(parm_dict, \n",
        "                      to_lower=True, \n",
        "                      remove_pontuacao=True,\n",
        "                      remove_stopwords=True,\n",
        "                      aplica_stemming=True,\n",
        "                      aplica_lematizacao=True):\n",
        "  \"\"\"\n",
        "  Recebe um dicionário e retorna uma cópia do mesmo com os valores da chave \"text\" pré-processados.\n",
        "\n",
        "  Args:\n",
        "  parm_dict (dict): Dicionário com chaves de texto e valores em texto.\n",
        "  to_lower (bool): Transforma o texto em caixa baixa. Padrão é True.\n",
        "  remove_pontuacao (bool): Remove a pontuação do texto. Padrão é True.\n",
        "  remove_stopwords (bool): Remove as palavras de parada do texto. Padrão é True.\n",
        "  aplica_stemming (bool): Aplica a técnica de stemming no texto. Padrão é True.\n",
        "  aplica_lematizacao (bool): Aplica a técnica de lematização no texto. Padrão é True.\n",
        "\n",
        "  Returns:\n",
        "  dict: Novo dicionário com a nova chave 'texto_prep' e seus valores pré-processados.\n",
        "\n",
        "  new_dict = dict(parm_dict)  # cria uma cópia do dicionário\n",
        "  for elemento in new_dict:\n",
        "    for key in new_dict[elemento]:\n",
        "      if key == \"text\":\n",
        "          new_dict[elemento][\"text_prep\"] = preprocessa_texto(new_dict[elemento][\"text\"], to_lower=to_lower, \n",
        "                                                    remove_pontuacao=remove_pontuacao,\n",
        "                                                    remove_stopwords=remove_stopwords,\n",
        "                                                    aplica_stemming=aplica_stemming,\n",
        "                                                    aplica_lematizacao=aplica_lematizacao)\n",
        "  return new_dict\n",
        "  \"\"\"\n",
        "  new_dict = {}\n",
        "  for elemento in parm_dict:\n",
        "    new_dict[elemento] = parm_dict[elemento].copy()\n",
        "    for key in parm_dict[elemento]:\n",
        "      if key == \"text\":\n",
        "          new_dict[elemento][\"text_prep\"] = preprocessa_texto(new_dict[elemento][\"text\"], to_lower=to_lower, \n",
        "                                                    remove_pontuacao=remove_pontuacao,\n",
        "                                                    remove_stopwords=remove_stopwords,\n",
        "                                                    aplica_stemming=aplica_stemming,\n",
        "                                                    aplica_lematizacao=aplica_lematizacao)\n",
        "  return new_dict"
      ],
      "metadata": {
        "id": "lWl3v9JEyA7E"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consultas_prep = preprocessa_texto_em_dict(consultas)"
      ],
      "metadata": {
        "id": "84XwZTRoyA32"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consultas_prep[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXkJAX-hyA0b",
        "outputId": "ae9cb921-eee1-4f03-e2b1-b40baa2dee96"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'What problems and concerns are there in making up descriptive titles?\\nWhat difficulties are involved in automatically retrieving articles from\\napproximate titles?\\nWhat is the usual relevance of the content of articles to their titles?',\n",
              " 'text_prep': ['problem',\n",
              "  'concern',\n",
              "  'make',\n",
              "  'descript',\n",
              "  'titl',\n",
              "  'difficulti',\n",
              "  'involv',\n",
              "  'automat',\n",
              "  'retriev',\n",
              "  'articl',\n",
              "  'approxim',\n",
              "  'titl',\n",
              "  'usual',\n",
              "  'relev',\n",
              "  'content',\n",
              "  'articl',\n",
              "  'titl']}"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_prep = preprocessa_texto_em_dict(documentos)"
      ],
      "metadata": {
        "id": "V54X2a7j1rLS"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_prep[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0xJM9wM1vpm",
        "outputId": "47ddeb0a-f1ec-4a2c-8e1c-22f95a3f0bf4"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': '18 Editions of the Dewey Decimal Classifications',\n",
              " 'author': 'Comaromi, J.P.',\n",
              " 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\",\n",
              " 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1',\n",
              " 'text_prep': ['present',\n",
              "  'studi',\n",
              "  'histori',\n",
              "  'dewey',\n",
              "  'decim',\n",
              "  'classif',\n",
              "  'first',\n",
              "  'edit',\n",
              "  'ddc',\n",
              "  'publish',\n",
              "  '1876',\n",
              "  'eighteenth',\n",
              "  'edit',\n",
              "  '1971',\n",
              "  'futur',\n",
              "  'edit',\n",
              "  'continu',\n",
              "  'appear',\n",
              "  'need',\n",
              "  'spite',\n",
              "  'ddc',\n",
              "  'long',\n",
              "  'healthi',\n",
              "  'life',\n",
              "  'howev',\n",
              "  'full',\n",
              "  'stori',\n",
              "  'never',\n",
              "  'told',\n",
              "  'biographi',\n",
              "  'dewey',\n",
              "  'briefli',\n",
              "  'describ',\n",
              "  'system',\n",
              "  'first',\n",
              "  'attempt',\n",
              "  'provid',\n",
              "  'detail',\n",
              "  'histori',\n",
              "  'work',\n",
              "  'spur',\n",
              "  'growth',\n",
              "  'librarianship',\n",
              "  'countri',\n",
              "  'abroad']}"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 3: Indexação"
      ],
      "metadata": {
        "id": "H9j_p52kx6MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Já apliquei o pré-processamento nos documentos e nas consultas da CISI Collection.\n",
        "\n",
        "Nossa próxima etapa é a indexação (com BM25) para posterior recuperação.\n",
        "\n",
        "Preciso de uma orientação sua para o que fazer agora?"
      ],
      "metadata": {
        "id": "6NRIRaKP14In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oswgnp65xL0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}