{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFWJVwb+eYrouxI+7xoM8g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sir_ChatGPT\n",
        "\n",
        "Development of a Simple Informational Retrieval System obtaining guidance with ChatGPT. Tested on CISI collection with BM25 algorithm.\n",
        "\n",
        "\n",
        "Autor: Marcus Vinícius Borela de Castro\n",
        "\n",
        "\n",
        "[Repositório no github](https://github.com/marcusborela/Sir-ChatGPT)"
      ],
      "metadata": {
        "id": "CcN_5-RDWeqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcusborela/Sir-ChatGPT/blob/main/code/Sir_ChatGPT.ipynb)"
      ],
      "metadata": {
        "id": "ti1aFWTVgejM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotinas utilitárias"
      ],
      "metadata": {
        "id": "2HsZlZ7YlPi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mostra_dict(dicionario: dict):\n",
        "    \"\"\"\n",
        "    Imprime informações sobre o dicionário recebido como parâmetro.\n",
        "\n",
        "    Argumentos:\n",
        "    - dicionario: um dicionário a ser impresso\n",
        "\n",
        "    Retorna:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # obtém a primeira e última chave do dicionário\n",
        "    primeiro_elemento = list(dicionario.keys())[0]\n",
        "    ultimo_elemento = list(dicionario.keys())[-1]\n",
        "\n",
        "    # imprime o tamanho do dicionário e as informações sobre seus limites\n",
        "    print(f\"O dicionário tem tamanho: {len(dicionario)}\")\n",
        "    print(f\"Seus limites:\\n {primeiro_elemento}:\\n {dicionario[primeiro_elemento]},\\n {ultimo_elemento}:\\n {dicionario[ultimo_elemento]}\")\n"
      ],
      "metadata": {
        "id": "ctvgk_oNlUPj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 1: Coleta dos dados"
      ],
      "metadata": {
        "id": "2L6Y2H8MH14s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baixar CISI Collection"
      ],
      "metadata": {
        "id": "sZ3pW9-8L6ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import io\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "6NSNoe3EI-gL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a URL da coleção CISI\n",
        "url = \"http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\"\n",
        "\n",
        "# Define o caminho onde o arquivo será salvo\n",
        "file_path = \"cisi.tar\"\n",
        "\n",
        "# Faz o download do arquivo\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Extrai os arquivos da coleção CISI\n",
        "with tarfile.open(file_path, \"r\") as tar:\n",
        "    tar.extractall()\n"
      ],
      "metadata": {
        "id": "-QFPkX52Jzts"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove('cisi.tar')"
      ],
      "metadata": {
        "id": "5wYycqvbMTGH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rotina para carregar arquivos da CISI Collection\n",
        "\n",
        "Os arquivos com consultas e documentos têm um formato padrão:\n",
        ".I <id>\n",
        ".tag\n",
        "valor tag\n",
        "\n",
        "E as tags podem ser:\n",
        "\n",
        "    .T = \"title\"\n",
        "    .A = \"author\"\n",
        "    .X = \"reference\"\n",
        "    .B = \"bibliograph\"\n",
        "    .W = \"text\""
      ],
      "metadata": {
        "id": "2UwCT9VIN2XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def carregar_cisi_arquivo(nome_arquivo:str, se_debug:bool=False):\n",
        "\n",
        "  # abre o arquivo CISI.ALL em modo de leitura\n",
        "  with open(nome_arquivo, \"r\") as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "  # inicializa o dicionário\n",
        "  documents = {}\n",
        "\n",
        "  # inicializa as variáveis de controle\n",
        "  doc_id = None\n",
        "  field = None\n",
        "  text = \"\"\n",
        "\n",
        "  # percorre as linhas do arquivo\n",
        "  for line in lines:\n",
        "    # se a linha começa com \".I\", então é um novo documento\n",
        "    if line.startswith((\".I\", \".T\", \".A\", \".W\", \".X\", \".B\")):\n",
        "      # faça algo se a linha começar com um desses valores\n",
        "        if doc_id is not None and field != \"id\":\n",
        "            documents[doc_id][field] = text.strip()\n",
        "            if se_debug:\n",
        "              print(f\"Atribuido a documents[{doc_id}][{field}] = {text.strip()}\")\n",
        "            text = \"\"\n",
        "    # se a linha começa com \".I\", então é um novo documento\n",
        "    if line.startswith(\".I\"):\n",
        "        # extrai o ID do documento da linha\n",
        "        doc_id = int(line.split()[1])\n",
        "        field = \"id\"\n",
        "        documents[doc_id] = {}\n",
        "        if se_debug:\n",
        "          print(f\"novo doc_id {doc_id}\")\n",
        "    # se a linha começa com \".T\", \".A\" ou \".W\", então é um novo campo\n",
        "    elif line.startswith(\".T\"):\n",
        "        field = \"title\"\n",
        "    elif line.startswith(\".A\"):\n",
        "        field = \"author\"\n",
        "    elif line.startswith(\".X\"):\n",
        "        field = \"reference\"\n",
        "    elif line.startswith(\".B\"):\n",
        "        field = \"bibliograph\"\n",
        "    elif line.startswith(\".W\"):\n",
        "        field = \"text\"\n",
        "    # caso contrário, é um texto que faz parte do campo atual\n",
        "    else:\n",
        "        text += line\n",
        "    if se_debug:\n",
        "      print(f'doc_id {doc_id} field {field}: {line}')\n",
        "      if doc_id == 4: break \n",
        "\n",
        "  # adiciona o último documento ao dicionário\n",
        "  if doc_id is not None:\n",
        "      documents[doc_id][field] = text.strip()\n",
        "\n",
        "  return documents"
      ],
      "metadata": {
        "id": "iv-OWG_3Le7o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregando Documentos - CISI.ALL"
      ],
      "metadata": {
        "id": "hQAxF00EXVJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentos = carregar_cisi_arquivo('CISI.ALL')"
      ],
      "metadata": {
        "id": "EgnfWs83SD8D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(documentos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOsLbLd8XM-w",
        "outputId": "e27583a7-8243-4813-ddbb-ca8ff0777ef4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 1460\n",
            "Seus limites:\n",
            " 1:\n",
            " {'title': '18 Editions of the Dewey Decimal Classifications', 'author': 'Comaromi, J.P.', 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\", 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1'},\n",
            " 1460:\n",
            " {'title': 'Modern Integral Information Systems for Chemistry and Chemical Technology', 'author': 'Chernyi, A.I.', 'text': \"At the present time, about 15% of all the world publications of \\nscientific and technical literature relate to chemistry and chemical\\ntechnology.  Each year throughout the world more than 250,000\\ndocuments are published:  journal papers, specifications for authors'\\ncertificates and patents, scientific and technical reports, monographs,\\netc., and in the last twenty years the number of such documents has\\nincreased by an average of 9% a year.  In these scientific documents\\ninformation on 100-150 thousand new chemical compounds is published.\", 'reference': '347\\t1\\t1460\\n452\\t1\\t1460\\n1095\\t1\\t1460\\n1136\\t1\\t1460\\n1223\\t1\\t1460\\n1261\\t1\\t1460\\n1285\\t1\\t1460\\n1460\\t6\\t1460\\n1460\\t6\\t1460'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregando Consultas - CISI.QRY"
      ],
      "metadata": {
        "id": "Ka-w4MufXeZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consultas = carregar_cisi_arquivo('CISI.QRY')"
      ],
      "metadata": {
        "id": "8sw1_cNLePHk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(consultas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEWBFttvj1bF",
        "outputId": "8f928f36-ad36-4a6b-e0bf-0c49468dfd77"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 112\n",
            "Seus limites:\n",
            " 1:\n",
            " {'text': 'What problems and concerns are there in making up descriptive titles?\\nWhat difficulties are involved in automatically retrieving articles from\\napproximate titles?\\nWhat is the usual relevance of the content of articles to their titles?'},\n",
            " 112:\n",
            " {'title': 'A Fast Procedure for the Calculation of Similarity Coefficients in\\nin Automatic Classification', 'author': 'Willett, P.', 'text': 'A fast algorithm is described for comparing the lists of terms representing\\ndocuments in automatic classification experiments.  The speed of the procedure\\narises from the fact that all of the non-zero-valued coefficicents for a given\\ndocument are identified together, using an inverted file to the terms in the\\ndocument collection.  The complexity and running time of the algorithm are\\ncompared with previously described procedures.', 'bibliograph': '(Information Processing & Management, Vol. 17, No. 2, 1981, pp. 53-60)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregando dados de relevância - CISI.REL"
      ],
      "metadata": {
        "id": "gFNnxXRGXlpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inicializa o dicionário de relevância por consulta\n",
        "relevancia_consulta = {}\n",
        "\n",
        "# abre o arquivo CISI.REL em modo de leitura\n",
        "with open('CISI.REL') as f:\n",
        "    # percorre as linhas do arquivo\n",
        "    for line in f.readlines():\n",
        "        # extrai o ID da consulta e do documento da linha\n",
        "        qry_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0])\n",
        "        doc_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
        "        \n",
        "        # adiciona o ID do documento na lista de relevância da consulta\n",
        "        if qry_id in relevancia_consulta:\n",
        "            relevancia_consulta[qry_id].append(doc_id)\n",
        "        else:\n",
        "            relevancia_consulta[qry_id] = []\n",
        "            relevancia_consulta[qry_id].append(doc_id)\n",
        "# Fonte de apoio: https://www.kaggle.com/code/razamh/some-changes-in-cisi-eda"
      ],
      "metadata": {
        "id": "MdN4v1nzg6AR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostra_dict(relevancia_consulta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZMSV96MeO5R",
        "outputId": "f9b5d74f-007f-4726-a5f2-2484f7eaf526"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O dicionário tem tamanho: 76\n",
            "Seus limites:\n",
            " 1:\n",
            " [28, 35, 38, 42, 43, 52, 65, 76, 86, 150, 189, 192, 193, 195, 215, 269, 291, 320, 429, 465, 466, 482, 483, 510, 524, 541, 576, 582, 589, 603, 650, 680, 711, 722, 726, 783, 813, 820, 868, 869, 894, 1162, 1164, 1195, 1196, 1281],\n",
            " 111:\n",
            " [328, 422, 448, 485, 503, 509]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HUtvq4hfLEVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 2: Pré-processamento dos textos"
      ],
      "metadata": {
        "id": "e007dYSoJ-zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "gu1iJ0WVWU3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "#stemmer = SnowballStemmer('english')\n"
      ],
      "metadata": {
        "id": "KLONEn8nlxPF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLgzi6qlt7UZ",
        "outputId": "939f9887-6d11-4708-9465-80b1dd5b01d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "SoFU0laIrmuY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código do pré-processador"
      ],
      "metadata": {
        "id": "IEtaYkm2YGGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessa_texto(texto,\n",
        "                      to_lower=True, \n",
        "                      remove_pontuacao=True,\n",
        "                      remove_stopwords=True,\n",
        "                      aplica_stemming=True,\n",
        "                      aplica_lematizacao=True):\n",
        "    \"\"\"\n",
        "    Função que realiza o pré-processamento de um texto.\n",
        "    \n",
        "    Parâmetros:\n",
        "    texto (str): Texto a ser pré-processado\n",
        "    to_lower (bool): Flag que indica se deve transformar o texto para lower case. Default: True\n",
        "    remove_pontuacao (bool): Flag que indica se deve remover a pontuação do texto. Default: True\n",
        "    remove_stopwords (bool): Flag que indica se deve remover as stop words do texto. Default: True\n",
        "    aplica_stemming (bool): Flag que indica se deve aplicar stemming no texto. Default: True\n",
        "    aplica_lematizacao (bool): Flag que indica se deve aplicar lematização no texto. Default: True\n",
        "    \n",
        "    Retorna:\n",
        "    str: Texto pré-processado\n",
        "    \"\"\"\n",
        "    # Transforma o texto em lower case\n",
        "    if to_lower:\n",
        "        texto = texto.lower()\n",
        "\n",
        "    # Substitute line breaks for space\n",
        "    texto = re.sub(r'\\n', ' ', texto)\n",
        "\n",
        "    # Remove pontuação\n",
        "    if remove_pontuacao:\n",
        "        texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    palavras = texto.split()\n",
        "\n",
        "    # Remove stop words\n",
        "    if remove_stopwords:\n",
        "        palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stop_words]\n",
        "        palavras = palavras_sem_stopwords\n",
        "\n",
        "    # Aplica stemming\n",
        "    if aplica_stemming:\n",
        "        palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras]\n",
        "        palavras = palavras_stemizadas\n",
        "\n",
        "    # Aplica lematização\n",
        "    if aplica_lematizacao:\n",
        "        palavras_lematizadas = [lemmatizer.lemmatize(palavra) for palavra in palavras]\n",
        "        palavras = palavras_lematizadas\n",
        "\n",
        "    return ' '.join(palavras)\n"
      ],
      "metadata": {
        "id": "06S5t8XQpCoT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessa_texto(\"This is an example of text.\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V2HwAWsksS1W",
        "outputId": "80c16879-0994-4d1b-d566-1af77a588652"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'exampl text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessa_texto_em_dict(parm_dict, \n",
        "                      to_lower=True, \n",
        "                      remove_pontuacao=True,\n",
        "                      remove_stopwords=True,\n",
        "                      aplica_stemming=True,\n",
        "                      aplica_lematizacao=True):\n",
        "  \"\"\"\n",
        "  Recebe um dicionário e retorna uma cópia do mesmo com os valores da chave \"text\" pré-processados.\n",
        "\n",
        "  Args:\n",
        "  parm_dict (dict): Dicionário com chaves de texto e valores em texto.\n",
        "  to_lower (bool): Transforma o texto em caixa baixa. Padrão é True.\n",
        "  remove_pontuacao (bool): Remove a pontuação do texto. Padrão é True.\n",
        "  remove_stopwords (bool): Remove as palavras de parada do texto. Padrão é True.\n",
        "  aplica_stemming (bool): Aplica a técnica de stemming no texto. Padrão é True.\n",
        "  aplica_lematizacao (bool): Aplica a técnica de lematização no texto. Padrão é True.\n",
        "\n",
        "  Returns:\n",
        "  dict: Novo dicionário com a nova chave 'texto_prep' e seus valores pré-processados.\n",
        "\n",
        "  new_dict = dict(parm_dict)  # cria uma cópia do dicionário\n",
        "  for elemento in new_dict:\n",
        "    for key in new_dict[elemento]:\n",
        "      if key == \"text\":\n",
        "          new_dict[elemento][\"text_prep\"] = preprocessa_texto(new_dict[elemento][\"text\"], to_lower=to_lower, \n",
        "                                                    remove_pontuacao=remove_pontuacao,\n",
        "                                                    remove_stopwords=remove_stopwords,\n",
        "                                                    aplica_stemming=aplica_stemming,\n",
        "                                                    aplica_lematizacao=aplica_lematizacao)\n",
        "  return new_dict\n",
        "  \"\"\"\n",
        "  new_dict = {}\n",
        "  for elemento in parm_dict:\n",
        "    new_dict[elemento] = parm_dict[elemento].copy()\n",
        "    for key in parm_dict[elemento]:\n",
        "      if key == \"text\":\n",
        "          new_dict[elemento][\"text_prep\"] = preprocessa_texto(new_dict[elemento][\"text\"], to_lower=to_lower, \n",
        "                                                    remove_pontuacao=remove_pontuacao,\n",
        "                                                    remove_stopwords=remove_stopwords,\n",
        "                                                    aplica_stemming=aplica_stemming,\n",
        "                                                    aplica_lematizacao=aplica_lematizacao)\n",
        "  return new_dict"
      ],
      "metadata": {
        "id": "lWl3v9JEyA7E"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teste do código de pre-processamento"
      ],
      "metadata": {
        "id": "bPWyOfASYCZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessa_texto(\"This is a simple text.\").split() == ['simpl', 'text']\n"
      ],
      "metadata": {
        "id": "pv_l-rptsSzJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessa_texto(\"Hello! My name is John. Nice to meet you!\").split() == ['hello', 'name', 'john', 'nice', 'meet']\n",
        "assert preprocessa_texto(\"We are learning about Natural Language Processing.\").split() == ['learn', 'natur', 'languag', 'process']\n",
        "assert preprocessa_texto(\"The quick brown fox jumps over the lazy dog.\").split() == ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "assert preprocessa_texto(\"To be, or not to be: that is the question.\").split() == [\"question\"]\n",
        "assert preprocessa_texto(\"I'm a developer at OpenAI. I love working with AI and NLP technologies!\").split() == ['im', 'develop', 'openai', 'love', 'work', 'ai', 'nlp', 'technolog']\n",
        "assert preprocessa_texto(\"The cat is on the mat.\").split() == ['cat', 'mat']\n",
        "assert preprocessa_texto(\"An investment in knowledge pays the best interest.\").split() == ['invest', 'knowledg', 'pay', 'best', 'interest']\n",
        "assert preprocessa_texto(\"The quick brown fox jumps over the lazy dog.\").split() == ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
        "assert preprocessa_texto(\"To be, or not to be: that is the question.\").split() == [\"question\"]\n",
        "assert preprocessa_texto(\"I'm a developer at OpenAI. I love working with AI and NLP technologies!\").split() == ['im', 'develop', 'openai', 'love', 'work', 'ai', 'nlp', 'technolog']\n",
        "assert preprocessa_texto(\"The cat is on the mat.\").split() == ['cat', 'mat']\n",
        "assert preprocessa_texto(\"An investment in knowledge pays the best interest.\").split() == ['invest', 'knowledg', 'pay', 'best', 'interest']\n"
      ],
      "metadata": {
        "id": "p0_NzgMIZ9NA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert preprocessa_texto(\"Hello World!\").split() == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello, World!!!\").split() == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello World\", remove_pontuacao=False).split() == ['hello', 'world']\n",
        "assert preprocessa_texto(\"Hello World\", remove_stopwords=False).split() == ['hello', 'world']\n",
        "assert preprocessa_texto(\"I am running in the park\", aplica_stemming=False).split() == ['running', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", to_lower=False, remove_stopwords=False, aplica_stemming=False, aplica_lematizacao=False).split() == ['I', 'am', 'running', 'in', 'the', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", aplica_lematizacao=False).split() == ['run', 'park']\n",
        "assert preprocessa_texto(\"I am running in the park\", to_lower=False).split() == ['i', 'run', 'park']\n"
      ],
      "metadata": {
        "id": "Ah6ynUi0qgpR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WsE6S-GyBBf",
        "outputId": "209a3d43-3db9-4e64-ad7d-a4d73f9b28c0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': '18 Editions of the Dewey Decimal Classifications',\n",
              " 'author': 'Comaromi, J.P.',\n",
              " 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\",\n",
              " 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1'}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pré-processamento da coleção"
      ],
      "metadata": {
        "id": "-OA77kPcYkdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consultas_prep = preprocessa_texto_em_dict(consultas)"
      ],
      "metadata": {
        "id": "84XwZTRoyA32"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consultas_prep[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXkJAX-hyA0b",
        "outputId": "48842cff-a2d7-41bb-854a-0c1b1b65321b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'What problems and concerns are there in making up descriptive titles?\\nWhat difficulties are involved in automatically retrieving articles from\\napproximate titles?\\nWhat is the usual relevance of the content of articles to their titles?',\n",
              " 'text_prep': 'problem concern make descript titl difficulti involv automat retriev articl approxim titl usual relev content articl titl'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_prep = preprocessa_texto_em_dict(documentos)"
      ],
      "metadata": {
        "id": "V54X2a7j1rLS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documentos_prep[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0xJM9wM1vpm",
        "outputId": "557b765f-457b-4497-b4d2-72ff4e5cc5e5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': '18 Editions of the Dewey Decimal Classifications',\n",
              " 'author': 'Comaromi, J.P.',\n",
              " 'text': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\",\n",
              " 'reference': '1\\t5\\t1\\n92\\t1\\t1\\n262\\t1\\t1\\n556\\t1\\t1\\n1004\\t1\\t1\\n1024\\t1\\t1\\n1024\\t1\\t1',\n",
              " 'text_prep': 'present studi histori dewey decim classif first edit ddc publish 1876 eighteenth edit 1971 futur edit continu appear need spite ddc long healthi life howev full stori never told biographi dewey briefli describ system first attempt provid detail histori work spur growth librarianship countri abroad'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 3: Indexação"
      ],
      "metadata": {
        "id": "H9j_p52kx6MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25"
      ],
      "metadata": {
        "id": "6NRIRaKP14In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b42887-160b-42df-c244-970ed9e90c5d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.8/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rank-bm25) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n"
      ],
      "metadata": {
        "id": "RTDwvNu6UeO8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "class BM25:\n",
        "    \n",
        "    def __init__(self, documents, k1=1.2, b=0.75, epsilon=0.25):\n",
        "        \"\"\"\n",
        "        Inicializa a classe BM25 com os documentos a serem indexados e os parâmetros do modelo.\n",
        "\n",
        "        Args:\n",
        "        documents (dict): Dicionário com chaves de texto e valores em texto pré-processado.\n",
        "        k1 (float): Parâmetro de ajuste para a frequência do termo (padrão é 1,2).\n",
        "        b (float): Parâmetro de ajuste para o comprimento do documento (padrão é 0,75).\n",
        "        epsilon (float): Parâmetro de suavização para evitar divisão por zero (padrão é 0,25).\n",
        "        \"\"\"\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        self.N = len(documents)\n",
        "        self.avgdl = sum([len(documents[doc][\"text_prep\"]) for doc in documents]) / self.N\n",
        "        self.f = []  # Lista vazia que será preenchida com as frequências dos termos para cada documento\n",
        "        self.df = {}  # Dicionário vazio que será preenchido com o número de documentos que contêm cada termo\n",
        "        self.idf = {}  # Dicionário vazio que será preenchido com os valores IDF de cada termo\n",
        "        self.documents = documents\n",
        "        self.criar_index()  # Cria o índice invertido dos documentos fornecidos\n",
        "    \n",
        "    def criar_index(self):\n",
        "        \"\"\"\n",
        "        Realiza a indexação dos documentos fornecidos no construtor da classe.\n",
        "        \"\"\"\n",
        "        for doc in self.documents:\n",
        "            frequencies = Counter(self.documents[doc][\"text_prep\"])  # Frequências dos termos no documento\n",
        "            document_length = len(self.documents[doc][\"text_prep\"])  # Comprimento do documento em termos de palavras\n",
        "            self.f.append(frequencies)  # Adiciona a frequência dos termos do documento na lista self.f\n",
        "            for word in frequencies:\n",
        "                if word not in self.df:\n",
        "                    self.df[word] = 1\n",
        "                else:\n",
        "                    self.df[word] += 1  # Atualiza o número de documentos que contêm o termo\n",
        "            for word, freq in frequencies.items():\n",
        "                if word not in self.idf:\n",
        "                    self.idf[word] = math.log((self.N - self.df[word] + 0.5) / (self.df[word] + 0.5))\n",
        "                    # Calcula e armazena o IDF de cada termo\n",
        "            #não gerou segunda vez: self.score(doc, document_length)\n",
        "\n",
        "    def score(self, document_id, document_length):\n",
        "        \"\"\"\n",
        "        Calcula o score BM25 para um documento.\n",
        "\n",
        "        Args:\n",
        "        document_id (str): ID do documento a ser calculado o score.\n",
        "        document_length (int): Comprimento do documento em termos de palavras.\n",
        "\n",
        "        Returns:\n",
        "        float: O score BM25 para o documento.\n",
        "        \"\"\"\n",
        "        score = 0  # Inicializa o score do documento com zero\n",
        "        for word in self.f[-1]:\n",
        "            freq = self.f[-1][word]\n",
        "            idf = self.idf[word]\n",
        "            num = idf * freq * (self.k1 + 1)\n",
        "            denom = freq + self.k1 * (1 - self.b + self.b * document_length / self.avgdl)\n",
        "            score += num / denom  # Calcula e adiciona o score do term\n",
        "        return score\n",
        "\n",
        "    def buscar(self, query, n=10):\n",
        "        \"\"\"\n",
        "        Realiza a busca de documentos utilizando a query fornecida.\n",
        "\n",
        "        Args:\n",
        "        query (str): A query de busca.\n",
        "        n (int): O número de documentos a serem retornados.\n",
        "\n",
        "        Returns:\n",
        "        list: Uma lista de tuplas com o ID do documento e seu score BM25 correspondente.\n",
        "        \"\"\"\n",
        "        query = query.split()\n",
        "        query = [word for word in query if word in self.idf.keys()]\n",
        "        query_freq = Counter(query)\n",
        "        query_weights = {}\n",
        "        for query_term, query_term_freq in query_freq.items():\n",
        "            idf = self.idf[query_term]\n",
        "            query_weights[query_term] = idf * query_term_freq * (self.k1 + 1) / (query_term_freq + self.epsilon)\n",
        "\n",
        "        scores = []\n",
        "        for document_id in self.documents:\n",
        "            document_length = len(self.documents[document_id][\"text_prep\"])\n",
        "            score = self.score(document_id, document_length)\n",
        "            doc_weights = self.f[-1]\n",
        "            for query_term, query_term_weight in query_weights.items():\n",
        "                if query_term in doc_weights:\n",
        "                    score += query_term_weight * (self.k1 + 1) * doc_weights[query_term] / (doc_weights[query_term] + self.k1 * (1 - self.b + self.b * document_length / self.avgdl))\n",
        "            scores.append((document_id, score))\n",
        "\n",
        "        return sorted(scores, key=lambda x: x[1], reverse=True)[:n]            \n"
      ],
      "metadata": {
        "id": "d31VXENrfIbt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Indexador:\n",
        "    def __init__(self, documentos_prep):\n",
        "        \"\"\"\n",
        "        Inicializa o indexador.\n",
        "\n",
        "        Args:\n",
        "            documentos_prep (dict): Dicionário com as informações dos documentos pré-processados.\n",
        "        \"\"\"\n",
        "        self.documentos_prep = documentos_prep\n",
        "        self.index = None\n",
        "        self.bm25 = None\n",
        "        self.doc_ids = None\n",
        "\n",
        "    def criar_index(self):\n",
        "        \"\"\"\n",
        "        Cria o índice invertido dos documentos.\n",
        "        \"\"\"\n",
        "        docs = [doc[\"text_prep\"] for doc in self.documentos_prep.values()]\n",
        "        self.index = BM25Okapi(docs)\n",
        "        self.doc_ids = list(self.documentos_prep.keys())\n",
        "\n",
        "    def buscar(self, consulta, top_k=10):\n",
        "        \"\"\"\n",
        "        Realiza a busca BM25 para a consulta.\n",
        "\n",
        "        Args:\n",
        "            consulta (str): Consulta a ser buscada.\n",
        "            top_k (int): Número máximo de documentos a serem retornados. Padrão é 10.\n",
        "\n",
        "        Returns:\n",
        "            list: Lista com os ids dos documentos mais relevantes para a consulta.\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            self.criar_index()\n",
        "\n",
        "        tokenized_consulta = consulta.split(\" \")\n",
        "        scores = self.index.get_scores(tokenized_consulta)\n",
        "        top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "        return [self.doc_ids[i] for i in top_k_indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "29e4h5zyVLgN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_bm25 = Indexador(documentos_prep)"
      ],
      "metadata": {
        "id": "L4kQ61HlUeL8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_bm25.criar_index()"
      ],
      "metadata": {
        "id": "BbH8r4imUeJG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_bm25.buscar(consultas_prep[1]['text_prep'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBT9GYs8UeF0",
        "outputId": "d3605b35-d1b7-42eb-e772-d8d589944c26"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[429, 722, 1299, 759, 65, 76, 38, 711, 603, 820]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_bm25 = BM25(documentos_prep)"
      ],
      "metadata": {
        "id": "SZaAaeMLZefZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_bm25.buscar(consultas_prep[1]['text_prep'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWK85N7azZB",
        "outputId": "fff736ed-4f66-430e-8d05-e26efd57ff5c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1288, 391.6020621373468),\n",
              " (1296, 391.4578213881378),\n",
              " (1284, 389.46340036696995),\n",
              " (1086, 388.6225558262266),\n",
              " (1301, 388.6225558262266),\n",
              " (1289, 388.4832065299011),\n",
              " (931, 388.34408127201857),\n",
              " (1279, 388.0664997728012),\n",
              " (1302, 387.9280419921482),\n",
              " (1290, 387.789805171247)]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5mbu6AzbGI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}